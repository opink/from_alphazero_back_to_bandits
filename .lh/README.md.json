{
    "sourceFile": "README.md",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 37,
            "patches": [
                {
                    "date": 1669278523511,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1669278567057,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n \n 什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\n \n 1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\n-2. UCB（Upper Confidence Bound）算法：均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5)\n+2. UCB（Upper Confidence Bound）算法：均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\n 3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守）\n 4. 完全朴素（极大似然）：先试验一些次，每个臂都有均值后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的。\n \n >算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？你可以选一种bandits算法来选bandits算法。\n"
                },
                {
                    "date": 1669279027711,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,11 @@\n >LinUCB算法：单纯的老虎机，他回报情况就是老虎机自己内部决定的，而在广告推荐领域，一个选择的回报，是由state_i（用户状态）和action_i一起决定的，如果我们能用feature来刻画state_i和action_i这一对儿（是不是到这儿才品出来有RL的味了。。），在选择之前通过feature预估每一个arm的期望回报及置信区间，那就合理多了。\n \n 利用UCB（最乐观选择，SCB是最悲观）来调整E-E trade off，节点探索的次数越多，置信区间越窄；每次选择置信度区间上界最大的节点max（feature_i + (k*ln(N)/1+Tn_i)^0.5）来平衡探索的次数和未探索的节点的。\n \n+对featrue_i我们可以加上MCTS，用加权分配，t为拉杆次数，开始时倾向于以预估的feature_i做指导，拉的越多越倾向使用MCTS给出的。max((1-r^(1/t))feature_i + r^(1/t)*UCT)\n \n+\n ### 4. 总的来说，AlphaGo Zero分为两个部分，一部分是MCTS（蒙特卡洛树搜索），一部分是神经网络。\n \n 我们是要抛弃人类棋谱的，学会如何下棋完全是通过自对弈来完成。\n \n"
                },
                {
                    "date": 1669279106113,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n \n 什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\n \n 1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\n-2. UCB（Upper Confidence Bound）算法：均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\n+2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\n 3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守）\n 4. 完全朴素（极大似然）：先试验一些次，每个臂都有均值后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的。\n \n >算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？你可以选一种bandits算法来选bandits算法。\n"
                },
                {
                    "date": 1669279242290,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n >LinUCB算法：单纯的老虎机，他回报情况就是老虎机自己内部决定的，而在广告推荐领域，一个选择的回报，是由state_i（用户状态）和action_i一起决定的，如果我们能用feature来刻画state_i和action_i这一对儿（是不是到这儿才品出来有RL的味了。。），在选择之前通过feature预估每一个arm的期望回报及置信区间，那就合理多了。\n \n 利用UCB（最乐观选择，SCB是最悲观）来调整E-E trade off，节点探索的次数越多，置信区间越窄；每次选择置信度区间上界最大的节点max（feature_i + (k*ln(N)/1+Tn_i)^0.5）来平衡探索的次数和未探索的节点的。\n \n-对featrue_i我们可以加上MCTS，用加权分配，t为拉杆次数，开始时倾向于以预估的feature_i做指导，拉的越多越倾向使用MCTS给出的。max((1-r^(1/t))feature_i + r^(1/t)*UCT)\n+原始alphaGo中利用一个V价值神经网络估计期望回报，对featrue_i我们可以加上MCTS，用加权分配，t为拉杆次数，开始时倾向于以预估的feature_i做指导，拉的越多越倾向使用MCTS给出的。max((1-r^(1/t))feature_i + r^(1/t)*UCT)\n \n \n ### 4. 总的来说，AlphaGo Zero分为两个部分，一部分是MCTS（蒙特卡洛树搜索），一部分是神经网络。\n \n"
                },
                {
                    "date": 1669279338973,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -26,9 +26,9 @@\n \n 原始alphaGo中利用一个V价值神经网络估计期望回报，对featrue_i我们可以加上MCTS，用加权分配，t为拉杆次数，开始时倾向于以预估的feature_i做指导，拉的越多越倾向使用MCTS给出的。max((1-r^(1/t))feature_i + r^(1/t)*UCT)\n \n \n-### 4. 总的来说，AlphaGo Zero分为两个部分，一部分是MCTS（蒙特卡洛树搜索），一部分是神经网络。\n+## 4. 总的来说，AlphaGo Zero分为两个部分，一部分是MCTS（蒙特卡洛树搜索），一部分是神经网络。\n \n 我们是要抛弃人类棋谱的，学会如何下棋完全是通过自对弈来完成。\n \n 过程是这样，首先生成棋谱，然后将棋谱作为输入训练神经网络，训练好的神经网络用来预测落子和胜率。\n@@ -73,5 +73,7 @@\n 上面说过，通过MCTS算出该下哪一步棋。然后接着再经过1600次模拟算出下一步棋，如此循环直到分出胜负，这样一整盘棋就下完了，这就是一次完整的自对弈过程，那么MCTS就相当于人在大脑中思考。我们把每步棋的局面s_t 、算出的action概率向量 π_t 和胜率z_t （就是返回值value）保存下来，作为棋谱数据训练神经网络。\n \n 训练目标是最小化预测胜率v和自我对弈的胜率z之间的误差，并使神经网络走子概率p与搜索概率π的相似度最大化。简单点说就是让神经网络的预测跟MCTS的搜索结果尽量接近。\n \n-MCTS和神经网络你中有我、我中有你，如此反复迭代，网络预测的更准确，MCTS的结果更强大。在AlphaGo Zero中，自我对弈是由以前所有迭代中最好的玩家生成的。每次训练迭代之后，与最好玩家对弈测量新玩家的能力；如果以55%的优势获胜，那么它将取代最好的玩家，而自我对弈将由这个新玩家产生。\n\\ No newline at end of file\n+MCTS和神经网络你中有我、我中有你，如此反复迭代，网络预测的更准确，MCTS的结果更强大。在AlphaGo Zero中，自我对弈是由以前所有迭代中最好的玩家生成的。每次训练迭代之后，与最好玩家对弈测量新玩家的能力；如果以55%的优势获胜，那么它将取代最好的玩家，而自我对弈将由这个新玩家产生。\n+\n+## 5. MuZero ： 在state被embeding后的状态隐空间中MCTS\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671959478427,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,79 +1,96 @@\n-# from_alphazero_back_to_bandits\n- 从实现alphazero开始，回溯到Exploit-Explor问题\n->强化学习：bandits和alphazero\n-\n-## 1.“通用（最小化总遗憾的策略）过程比结果重要”\n-\n->问题：如果做（Exploit-Explore trade off）算力聚焦的时候，有一片区域在你的探索过程压根就探索不到的情况下\n-\n-## 2.选择是一个技术活。专治选择困难症——bandits算法\n-\n-什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\n-\n-1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\n-2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\n-3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守）\n-4. 完全朴素（极大似然）：先试验一些次，每个臂都有均值后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的。\n-\n->算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？你可以选一种bandits算法来选bandits算法。\n-\n-## 3.用bandits算法解决推荐系统冷启动的简单思路\n-\n-我想，屏幕前的你已经想到了，推荐系统冷启动可以用bandits算法来解决一部分。\n->LinUCB算法：单纯的老虎机，他回报情况就是老虎机自己内部决定的，而在广告推荐领域，一个选择的回报，是由state_i（用户状态）和action_i一起决定的，如果我们能用feature来刻画state_i和action_i这一对儿（是不是到这儿才品出来有RL的味了。。），在选择之前通过feature预估每一个arm的期望回报及置信区间，那就合理多了。\n-\n-利用UCB（最乐观选择，SCB是最悲观）来调整E-E trade off，节点探索的次数越多，置信区间越窄；每次选择置信度区间上界最大的节点max（feature_i + (k*ln(N)/1+Tn_i)^0.5）来平衡探索的次数和未探索的节点的。\n-\n-原始alphaGo中利用一个V价值神经网络估计期望回报，对featrue_i我们可以加上MCTS，用加权分配，t为拉杆次数，开始时倾向于以预估的feature_i做指导，拉的越多越倾向使用MCTS给出的。max((1-r^(1/t))feature_i + r^(1/t)*UCT)\n-\n-\n-## 4. 总的来说，AlphaGo Zero分为两个部分，一部分是MCTS（蒙特卡洛树搜索），一部分是神经网络。\n-\n-我们是要抛弃人类棋谱的，学会如何下棋完全是通过自对弈来完成。\n-\n-过程是这样，首先生成棋谱，然后将棋谱作为输入训练神经网络，训练好的神经网络用来预测落子和胜率。\n-\n-MCTS就是用来自对弈生成棋谱的，\n->a. 每次模拟通过选择具有最大行动价值Q的边加上取决于所存储的先验概率P和该边的访问计数N（每次访问都被增加一次）的上限置信区间U来遍历树。\n-b. 展开叶子节点，通过神经网络来评估局面s；向量P的值存储在叶子结点扩展的边上。\n-c. 更新行动价值Q等于在该行动下的子树中的所有评估值V的均值。\n-d. 一旦MCTS搜索完成，返回局面s下的落子概率π，与N^(1/τ)成正比，其中N是从根状态每次移动的访问计数， τ是控制温度的参数。\n-\n-按照论文所述，每次MCTS使用1600次模拟。过程是这样的，现在AI从白板一块开始自己跟自己下棋，只知道规则，不知道套路，那只好乱下。每下一步棋，都要通过MCTS模拟1600次上图中的a~c，从而得出我这次要怎么走子。\n-来说说a~c，MCTS本质上是我们来维护一棵树，这棵树的每个节点保存了每一个局面（situation）该如何走子（action）的信息。这些信息是，N(s, a)是访问次数，W(s, a)是总行动价值，Q(s, a)是平均行动价值，P(s, a)是被选择的概率。\n-\n-##### 4.1.1 a.Select\n-每次模拟的过程都一样，从父节点的局面开始，选择一个走子。比如开局的时候，所有合法的走子都是可能的选择，那么我该选哪个走子呢？这就是select要做的事情。MCTS选择Q(s, a) + U(s, a)最大的那个action。\n-\n-- U(s, a) = c_puct × 概率P(s, a) × np.sqrt(父节点访问次数N) / ( 1 + 某子节点action的访问次数N(s, a) )\n-- feature_i = Q(s_t,a)   ：把当前的局面作为输入传给神经网络，神经网络会返回给我们一个action向量p和当前胜率v。\n-\n-用论文中的话说，c_puct是一个决定探索水平的常数；这种搜索控制策略最初倾向于具有高先验概率和低访问次数的行为，但是渐近地倾向于具有高行动价值的行为。\n-计算过后，我就知道当前局面下，哪个action的Q+U值最大，那这个action走子之后的局面就是第二次模拟的当前局面。比如开局，Q+U最大的是当头炮，然后我就Select当头炮这个action，再下一次Select就从当头炮的这个棋局选择下一个走子。\n-\n-##### 4.1.2 b.Expand\n-现在开始第二次模拟了，假如之前的action是当头炮，我们要接着这个局面选择action，但是这个局面是个叶子节点。就是说当头炮之后可以选择哪些action不知道，这样就需要expand了，通过expand得到一系列可能的action节点。这样实际上就是在扩展这棵树，从只有根节点开始，一点一点的扩展。\n-\n-##### 4.1.3 c.Evaluate（fast rollout）\n-我们使用虚拟损失来确保每个线程评估不同的节点。\n-因为我们是多线程同时在做MCTS，由于Select算法都一样，都是选择Q+U最大节点，所以很有可能所有的线程最终选择的是同一个节点，这就尴尬了。我们的目的是尽可能在树上搜索出各种不同的着法，最终选择一步好棋，怎么办呢？论文中已经给出了办法，“我们使用虚拟损失来确保每个线程评估不同的节点。”就是说，通过Select选出某节点后，人为增大这个节点的访问次数N，并减少节点的总行动价值W，因为平均行动价值Q = W / N，这样分子减少，分母增加，就减少了Q值，这样递归进行的时候，此节点的Q+U不是最大，避免被选中，让其他的线程尝试选择别的节点进行树搜索。这个人为增加和减少的量就是虚拟损失virtual loss。\n-\n-##### 4.1.4 c.Backup\n-现在MCTS的过程越来越清晰了，Select选择节点，选择后，对当前节点使用虚拟损失，通过递归继续Select，直到分出胜负或Expand节点，得到返回值value。现在就可以使用value进行Backup了，但首先要还原W和N，之前N增加了虚拟损失，这次要减回去，之前减少了虚拟损失的W也要加回来。然后开始做Backup.\n-\n-\n-#### d. Play\n-按照上述过程执行a~c，论文中是每步棋执行1600次模拟，那就是1600次的a~c，这个MCTS的过程就是模拟自我对弈的过程。模拟结束后，基本上能覆盖大多数的棋局和着法，每步棋该怎么下，下完以后胜率是多少，得到什么样的局面都能在树上找到。然后从树上选择当前局面应该下哪一步棋，这就是步骤d.play:\"在搜索结束时，AlphaGo Zero在根节点s0选择一个走子a，与其访问计数幂指数成正比.\n-\n-在随后的时间步重新使用搜索树：与所走子的动作对应的子节点成为新的根节点；保留这个节点下面的子树所有的统计信息，而树的其余部分被丢弃。\n-\n-当模拟结束后，对于当前局面（就是树的根节点）的所有子节点就是每一步对应的action节点，选择哪一个action呢？按照论文所说是通过访问计数N来确定的。这个好理解吧？实现上也容易，当前节点的所有节点是可以获得的，每个子节点的信息N都可以获得，然后从多个action中选一个\n-\n-#### NerosNetwork\n-上面说过，通过MCTS算出该下哪一步棋。然后接着再经过1600次模拟算出下一步棋，如此循环直到分出胜负，这样一整盘棋就下完了，这就是一次完整的自对弈过程，那么MCTS就相当于人在大脑中思考。我们把每步棋的局面s_t 、算出的action概率向量 π_t 和胜率z_t （就是返回值value）保存下来，作为棋谱数据训练神经网络。\n-\n-训练目标是最小化预测胜率v和自我对弈的胜率z之间的误差，并使神经网络走子概率p与搜索概率π的相似度最大化。简单点说就是让神经网络的预测跟MCTS的搜索结果尽量接近。\n-\n-MCTS和神经网络你中有我、我中有你，如此反复迭代，网络预测的更准确，MCTS的结果更强大。在AlphaGo Zero中，自我对弈是由以前所有迭代中最好的玩家生成的。每次训练迭代之后，与最好玩家对弈测量新玩家的能力；如果以55%的优势获胜，那么它将取代最好的玩家，而自我对弈将由这个新玩家产生。\n-\n-## 5. MuZero ： 在state被embeding后的状态隐空间中MCTS\n\\ No newline at end of file\n+# from_alphazero_back_to_bandits\r\n+ 从实现alphazero开始，回溯到Exploit-Explor问题\r\n+>强化学习：bandits和alphazero\r\n+\r\n+## 1.“通用（最小化总遗憾的策略）过程比结果重要”\r\n+\r\n+>问题：如果做（Exploit-Explore trade off）算力聚焦的时候，有一片区域在你的探索过程压根就探索不到的情况下\r\n+\r\n+## 2.选择是一个技术活。专治选择困难症——bandits算法\r\n+\r\n+什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\r\n+\r\n+1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\r\n+2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\r\n+3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守）\r\n+4. 完全朴素（极大似然）：先试验一些次，每个臂都有均值后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的。\r\n+\r\n+>算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？（你可以选一种bandits算法来选bandits算法。）\r\n+\r\n+## 3.用bandits算法解决推荐系统冷启动的简单思路\r\n+\r\n+我想，屏幕前的你已经想到了，推荐系统冷启动可以用bandits算法来解决一部分。\r\n+>LinUCB算法：单纯的老虎机，他回报情况就是老虎机自己内部决定的，而在广告推荐领域，一个选择的回报，是由state_i（用户状态）和action_i一起决定的，如果我们能用feature来刻画state_i和action_i这一对儿（是不是到这儿才品出来有RL的味了。。），在选择之前通过feature预估每一个arm的期望回报及置信区间，那就合理多了。\r\n+\r\n+利用UCB（最乐观选择，SCB是最悲观）来调整E-E trade off，节点探索的次数越多，置信区间越窄；每次选择置信度区间上界最大的节点max（feature_i + (k*ln(N)/1+Tn_i)^0.5）来平衡探索的次数和未探索的节点的。\r\n+\r\n+原始alphaGo中利用一个V价值神经网络估计期望回报，对featrue_i我们可以加上MCTS，用加权分配，t为拉杆次数，开始时倾向于以预估的feature_i做指导，拉的越多越倾向使用MCTS给出的。max((1-r^(1/t))feature_i + r^(1/t)*UCT)\r\n+\r\n+\r\n+## 4. 总的来说，AlphaGo Zero分为两个部分，一部分是MCTS（蒙特卡洛树搜索），一部分是神经网络。\r\n+\r\n+我们是要抛弃人类棋谱的，学会如何下棋完全是通过自对弈来完成。\r\n+\r\n+过程是这样，首先生成棋谱，然后将棋谱作为输入训练神经网络，训练好的神经网络用来预测落子和胜率。\r\n+\r\n+MCTS就是用来自对弈生成棋谱的，\r\n+>a. 每次模拟通过选择具有最大行动价值Q的边加上取决于所存储的先验概率P和该边的访问计数N（每次访问都被增加一次）的上限置信区间U来遍历树。\r\n+b. 展开叶子节点，通过神经网络来评估局面s；向量P的值存储在叶子结点扩展的边上。\r\n+c. 更新行动价值Q等于在该行动下的子树中的所有评估值V的均值。\r\n+d. 一旦MCTS搜索完成，返回局面s下的落子概率π，与N^(1/τ)成正比，其中N是从根状态每次移动的访问计数， τ是控制温度的参数。\r\n+\r\n+按照论文所述，每次MCTS使用1600次模拟。过程是这样的，现在AI从白板一块开始自己跟自己下棋，只知道规则，不知道套路，那只好乱下。每下一步棋，都要通过MCTS模拟1600次上图中的a~c，从而得出我这次要怎么走子。\r\n+来说说a~c，MCTS本质上是我们来维护一棵树，这棵树的每个节点保存了每一个局面（situation）该如何走子（action）的信息。这些信息是，N(s, a)是访问次数，W(s, a)是总行动价值，Q(s, a)是平均行动价值，P(s, a)是被选择的概率。\r\n+\r\n+##### 4.1.1 a.Select\r\n+每次模拟的过程都一样，从父节点的局面开始，选择一个走子。比如开局的时候，所有合法的走子都是可能的选择，那么我该选哪个走子呢？这就是select要做的事情。MCTS选择Q(s, a) + U(s, a)最大的那个action。\r\n+\r\n+- U(s, a) = c_puct × 概率P(s, a) × np.sqrt(父节点访问次数N) / ( 1 + 某子节点action的访问次数N(s, a) )\r\n+- feature_i = Q(s_t,a)   ：把当前的局面作为输入传给神经网络，神经网络会返回给我们一个action向量p和当前胜率v。\r\n+\r\n+用论文中的话说，c_puct是一个决定探索水平的常数；这种搜索控制策略最初倾向于具有高先验概率和低访问次数的行为，但是渐近地倾向于具有高行动价值的行为。\r\n+计算过后，我就知道当前局面下，哪个action的Q+U值最大，那这个action走子之后的局面就是第二次模拟的当前局面。比如开局，Q+U最大的是当头炮，然后我就Select当头炮这个action，再下一次Select就从当头炮的这个棋局选择下一个走子。\r\n+\r\n+##### 4.1.2 b.Expand\r\n+现在开始第二次模拟了，假如之前的action是当头炮，我们要接着这个局面选择action，但是这个局面是个叶子节点。就是说当头炮之后可以选择哪些action不知道，这样就需要expand了，通过expand得到一系列可能的action节点。这样实际上就是在扩展这棵树，从只有根节点开始，一点一点的扩展。\r\n+\r\n+##### 4.1.3 c.Evaluate（fast rollout）\r\n+我们使用虚拟损失来确保每个线程评估不同的节点。\r\n+因为我们是多线程同时在做MCTS，由于Select算法都一样，都是选择Q+U最大节点，所以很有可能所有的线程最终选择的是同一个节点，这就尴尬了。我们的目的是尽可能在树上搜索出各种不同的着法，最终选择一步好棋，怎么办呢？论文中已经给出了办法，“我们使用虚拟损失来确保每个线程评估不同的节点。”就是说，通过Select选出某节点后，人为增大这个节点的访问次数N，并减少节点的总行动价值W，因为平均行动价值Q = W / N，这样分子减少，分母增加，就减少了Q值，这样递归进行的时候，此节点的Q+U不是最大，避免被选中，让其他的线程尝试选择别的节点进行树搜索。这个人为增加和减少的量就是虚拟损失virtual loss。\r\n+\r\n+##### 4.1.4 c.Backup\r\n+现在MCTS的过程越来越清晰了，Select选择节点，选择后，对当前节点使用虚拟损失，通过递归继续Select，直到分出胜负或Expand节点，得到返回值value。现在就可以使用value进行Backup了，但首先要还原W和N，之前N增加了虚拟损失，这次要减回去，之前减少了虚拟损失的W也要加回来。然后开始做Backup.\r\n+\r\n+\r\n+#### d. Play\r\n+按照上述过程执行a~c，论文中是每步棋执行1600次模拟，那就是1600次的a~c，这个MCTS的过程就是模拟自我对弈的过程。模拟结束后，基本上能覆盖大多数的棋局和着法，每步棋该怎么下，下完以后胜率是多少，得到什么样的局面都能在树上找到。然后从树上选择当前局面应该下哪一步棋，这就是步骤d.play:\"在搜索结束时，AlphaGo Zero在根节点s0选择一个走子a，与其访问计数幂指数成正比.\r\n+\r\n+在随后的时间步重新使用搜索树：与所走子的动作对应的子节点成为新的根节点；保留这个节点下面的子树所有的统计信息，而树的其余部分被丢弃。\r\n+\r\n+当模拟结束后，对于当前局面（就是树的根节点）的所有子节点就是每一步对应的action节点，选择哪一个action呢？按照论文所说是通过访问计数N来确定的。这个好理解吧？实现上也容易，当前节点的所有节点是可以获得的，每个子节点的信息N都可以获得，然后从多个action中选一个\r\n+\r\n+#### NerosNetwork\r\n+上面说过，通过MCTS算出该下哪一步棋。然后接着再经过1600次模拟算出下一步棋，如此循环直到分出胜负，这样一整盘棋就下完了，这就是一次完整的自对弈过程，那么MCTS就相当于人在大脑中思考。我们把每步棋的局面s_t 、算出的action概率向量 π_t 和胜率z_t （就是返回值value）保存下来，作为棋谱数据训练神经网络。\r\n+\r\n+训练目标是最小化预测胜率v和自我对弈的胜率z之间的误差，并使神经网络走子概率p与搜索概率π的相似度最大化。简单点说就是让神经网络的预测跟MCTS的搜索结果尽量接近。\r\n+\r\n+MCTS和神经网络你中有我、我中有你，如此反复迭代，网络预测的更准确，MCTS的结果更强大。在AlphaGo Zero中，自我对弈是由以前所有迭代中最好的玩家生成的。每次训练迭代之后，与最好玩家对弈测量新玩家的能力；如果以55%的优势获胜，那么它将取代最好的玩家，而自我对弈将由这个新玩家产生。\r\n+\r\n+# 到Muzero，对比学习\r\n+\r\n+看到Muzero和它的改进EfficientZero，发现同对比学习里的BYOL、Sim-Siam太像了。\r\n+也确实\r\n+\r\n+## 5. MuZero ： 在state被embeding后的状态隐空间中MCTS\r\n+1. 只有在根节点处，muzero会排除规则不允许的行动。 而在之后的隐空间中mcts就没有限制了（“重要的是错误。”这意味着可以进行自监督的对比学习了）\r\n+2. 正式走子时一样，选择mcts搜索后第一层最大的N\r\n+3. （人类使用图进行推理时，其实是在解码或“翻译”embeding到神经网络空间里的 算子）既然是在隐空间内plan，隐空间也可以再 投射（生成） 到graph的空间里\r\n+4. 这就打通了脑子了\r\n+\r\n+\r\n+## 6.对比学习(只要能定义正负例就可以自监督学习)\r\n+1. 那大脑其实也在做一样的事，增强现实（同时，也意味着，意识的虚假）\r\n+2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n+3. 错误提供信息，或者说，错误和规则等价\r\n+4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是穿起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n+5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。\r\n"
                },
                {
                    "date": 1671959490772,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,9 @@\n 2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\r\n 3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守）\r\n 4. 完全朴素（极大似然）：先试验一些次，每个臂都有均值后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的。\r\n \r\n->算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？（你可以选一种bandits算法来选bandits算法。）\r\n+>算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？（你可以选一种bandits算法来选bandits算法：）\r\n \r\n ## 3.用bandits算法解决推荐系统冷启动的简单思路\r\n \r\n 我想，屏幕前的你已经想到了，推荐系统冷启动可以用bandits算法来解决一部分。\r\n"
                },
                {
                    "date": 1671959906687,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n \r\n 1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\r\n 2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\r\n 3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守）\r\n-4. 完全朴素（极大似然）：先试验一些次，每个臂都有均值后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的。\r\n+4. 完全朴素（极大似然）：先平等的试验k次，k轮后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的（问题在于k太大影响收益，k小导致严重的过拟合）。\r\n \r\n >算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？（你可以选一种bandits算法来选bandits算法：）\r\n \r\n ## 3.用bandits算法解决推荐系统冷启动的简单思路\r\n"
                },
                {
                    "date": 1671959931140,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n 什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\r\n \r\n 1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\r\n 2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\r\n-3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守）\r\n+3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守。这个算法也是我们人类大脑可以模拟的基线了）\r\n 4. 完全朴素（极大似然）：先平等的试验k次，k轮后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的（问题在于k太大影响收益，k小导致严重的过拟合）。\r\n \r\n >算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？（你可以选一种bandits算法来选bandits算法：）\r\n \r\n"
                },
                {
                    "date": 1671959937336,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n 什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\r\n \r\n 1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\r\n 2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\r\n-3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守。这个算法也是我们人类大脑可以模拟的基线了）\r\n+3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守。这个算法也是我们人类大脑可以模拟的极限了）\r\n 4. 完全朴素（极大似然）：先平等的试验k次，k轮后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的（问题在于k太大影响收益，k小导致严重的过拟合）。\r\n \r\n >算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？（你可以选一种bandits算法来选bandits算法：）\r\n \r\n"
                },
                {
                    "date": 1671960025090,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n \r\n 什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\r\n \r\n 1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\r\n-2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2.\r\n+2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2。UCB算法讲究的就是一个乐观，我尽可能的闲心这个ARM能做到它的上确界，正是这种乐观，UCB最后往往都能收敛到最优。\r\n 3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守。这个算法也是我们人类大脑可以模拟的极限了）\r\n 4. 完全朴素（极大似然）：先平等的试验k次，k轮后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的（问题在于k太大影响收益，k小导致严重的过拟合）。\r\n \r\n >算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？（你可以选一种bandits算法来选bandits算法：）\r\n"
                },
                {
                    "date": 1671960387533,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n \r\n 什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\r\n \r\n 1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\r\n-2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2。UCB算法讲究的就是一个乐观，我尽可能的闲心这个ARM能做到它的上确界，正是这种乐观，UCB最后往往都能收敛到最优。\r\n+2. UCB（Upper Confidence Bound）算法：平均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5) k正态时选2。主要是抓住了随着试验进行，置信度区间的变动。UCB算法讲究的就是一个乐观，我尽可能的闲心这个ARM能做到它的上确界，正是这种乐观，UCB最后往往都能收敛到最优。\r\n 3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守。这个算法也是我们人类大脑可以模拟的极限了）\r\n 4. 完全朴素（极大似然）：先平等的试验k次，k轮后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的（问题在于k太大影响收益，k小导致严重的过拟合）。\r\n \r\n >算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？（你可以选一种bandits算法来选bandits算法：）\r\n"
                },
                {
                    "date": 1671961042693,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,55 +28,21 @@\n \r\n \r\n ## 4. 总的来说，AlphaGo Zero分为两个部分，一部分是MCTS（蒙特卡洛树搜索），一部分是神经网络。\r\n \r\n-我们是要抛弃人类棋谱的，学会如何下棋完全是通过自对弈来完成。\r\n+### 4.1 原始的（ALphaGo的）\r\n+#### 4.1.1 MCTS部分\r\n+1. 蒙特卡洛：fastrollout\r\n+2. 树搜索：select-expand-backup\r\n \r\n-过程是这样，首先生成棋谱，然后将棋谱作为输入训练神经网络，训练好的神经网络用来预测落子和胜率。\r\n+共四个步骤：select-expand-fastrollout-backup\r\n \r\n-MCTS就是用来自对弈生成棋谱的，\r\n->a. 每次模拟通过选择具有最大行动价值Q的边加上取决于所存储的先验概率P和该边的访问计数N（每次访问都被增加一次）的上限置信区间U来遍历树。\r\n-b. 展开叶子节点，通过神经网络来评估局面s；向量P的值存储在叶子结点扩展的边上。\r\n-c. 更新行动价值Q等于在该行动下的子树中的所有评估值V的均值。\r\n-d. 一旦MCTS搜索完成，返回局面s下的落子概率π，与N^(1/τ)成正比，其中N是从根状态每次移动的访问计数， τ是控制温度的参数。\r\n+#### 4.1.2 神经网络部分\r\n+1. 首先使用模仿学习先监督学出一个弱鸡人类策略网络P_0，\r\n+2. 而后用P_0辅助fastrollout，\r\n+3. 收集到一些对局轨迹之后，开始硬训练个更好的策略价值网络P，\r\n \r\n-按照论文所述，每次MCTS使用1600次模拟。过程是这样的，现在AI从白板一块开始自己跟自己下棋，只知道规则，不知道套路，那只好乱下。每下一步棋，都要通过MCTS模拟1600次上图中的a~c，从而得出我这次要怎么走子。\r\n-来说说a~c，MCTS本质上是我们来维护一棵树，这棵树的每个节点保存了每一个局面（situation）该如何走子（action）的信息。这些信息是，N(s, a)是访问次数，W(s, a)是总行动价值，Q(s, a)是平均行动价值，P(s, a)是被选择的概率。\r\n \r\n-##### 4.1.1 a.Select\r\n-每次模拟的过程都一样，从父节点的局面开始，选择一个走子。比如开局的时候，所有合法的走子都是可能的选择，那么我该选哪个走子呢？这就是select要做的事情。MCTS选择Q(s, a) + U(s, a)最大的那个action。\r\n-\r\n-- U(s, a) = c_puct × 概率P(s, a) × np.sqrt(父节点访问次数N) / ( 1 + 某子节点action的访问次数N(s, a) )\r\n-- feature_i = Q(s_t,a)   ：把当前的局面作为输入传给神经网络，神经网络会返回给我们一个action向量p和当前胜率v。\r\n-\r\n-用论文中的话说，c_puct是一个决定探索水平的常数；这种搜索控制策略最初倾向于具有高先验概率和低访问次数的行为，但是渐近地倾向于具有高行动价值的行为。\r\n-计算过后，我就知道当前局面下，哪个action的Q+U值最大，那这个action走子之后的局面就是第二次模拟的当前局面。比如开局，Q+U最大的是当头炮，然后我就Select当头炮这个action，再下一次Select就从当头炮的这个棋局选择下一个走子。\r\n-\r\n-##### 4.1.2 b.Expand\r\n-现在开始第二次模拟了，假如之前的action是当头炮，我们要接着这个局面选择action，但是这个局面是个叶子节点。就是说当头炮之后可以选择哪些action不知道，这样就需要expand了，通过expand得到一系列可能的action节点。这样实际上就是在扩展这棵树，从只有根节点开始，一点一点的扩展。\r\n-\r\n-##### 4.1.3 c.Evaluate（fast rollout）\r\n-我们使用虚拟损失来确保每个线程评估不同的节点。\r\n-因为我们是多线程同时在做MCTS，由于Select算法都一样，都是选择Q+U最大节点，所以很有可能所有的线程最终选择的是同一个节点，这就尴尬了。我们的目的是尽可能在树上搜索出各种不同的着法，最终选择一步好棋，怎么办呢？论文中已经给出了办法，“我们使用虚拟损失来确保每个线程评估不同的节点。”就是说，通过Select选出某节点后，人为增大这个节点的访问次数N，并减少节点的总行动价值W，因为平均行动价值Q = W / N，这样分子减少，分母增加，就减少了Q值，这样递归进行的时候，此节点的Q+U不是最大，避免被选中，让其他的线程尝试选择别的节点进行树搜索。这个人为增加和减少的量就是虚拟损失virtual loss。\r\n-\r\n-##### 4.1.4 c.Backup\r\n-现在MCTS的过程越来越清晰了，Select选择节点，选择后，对当前节点使用虚拟损失，通过递归继续Select，直到分出胜负或Expand节点，得到返回值value。现在就可以使用value进行Backup了，但首先要还原W和N，之前N增加了虚拟损失，这次要减回去，之前减少了虚拟损失的W也要加回来。然后开始做Backup.\r\n-\r\n-\r\n-#### d. Play\r\n-按照上述过程执行a~c，论文中是每步棋执行1600次模拟，那就是1600次的a~c，这个MCTS的过程就是模拟自我对弈的过程。模拟结束后，基本上能覆盖大多数的棋局和着法，每步棋该怎么下，下完以后胜率是多少，得到什么样的局面都能在树上找到。然后从树上选择当前局面应该下哪一步棋，这就是步骤d.play:\"在搜索结束时，AlphaGo Zero在根节点s0选择一个走子a，与其访问计数幂指数成正比.\r\n-\r\n-在随后的时间步重新使用搜索树：与所走子的动作对应的子节点成为新的根节点；保留这个节点下面的子树所有的统计信息，而树的其余部分被丢弃。\r\n-\r\n-当模拟结束后，对于当前局面（就是树的根节点）的所有子节点就是每一步对应的action节点，选择哪一个action呢？按照论文所说是通过访问计数N来确定的。这个好理解吧？实现上也容易，当前节点的所有节点是可以获得的，每个子节点的信息N都可以获得，然后从多个action中选一个\r\n-\r\n-#### NerosNetwork\r\n-上面说过，通过MCTS算出该下哪一步棋。然后接着再经过1600次模拟算出下一步棋，如此循环直到分出胜负，这样一整盘棋就下完了，这就是一次完整的自对弈过程，那么MCTS就相当于人在大脑中思考。我们把每步棋的局面s_t 、算出的action概率向量 π_t 和胜率z_t （就是返回值value）保存下来，作为棋谱数据训练神经网络。\r\n-\r\n-训练目标是最小化预测胜率v和自我对弈的胜率z之间的误差，并使神经网络走子概率p与搜索概率π的相似度最大化。简单点说就是让神经网络的预测跟MCTS的搜索结果尽量接近。\r\n-\r\n-MCTS和神经网络你中有我、我中有你，如此反复迭代，网络预测的更准确，MCTS的结果更强大。在AlphaGo Zero中，自我对弈是由以前所有迭代中最好的玩家生成的。每次训练迭代之后，与最好玩家对弈测量新玩家的能力；如果以55%的优势获胜，那么它将取代最好的玩家，而自我对弈将由这个新玩家产生。\r\n-\r\n # 到Muzero，对比学习\r\n \r\n 看到Muzero和它的改进EfficientZero，发现同对比学习里的BYOL、Sim-Siam太像了。\r\n 也确实\r\n"
                },
                {
                    "date": 1671961098005,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,8 +34,9 @@\n 1. 蒙特卡洛：fastrollout\r\n 2. 树搜索：select-expand-backup\r\n \r\n 共四个步骤：select-expand-fastrollout-backup\r\n+>UCT（UCB在树上的版本）\r\n \r\n #### 4.1.2 神经网络部分\r\n 1. 首先使用模仿学习先监督学出一个弱鸡人类策略网络P_0，\r\n 2. 而后用P_0辅助fastrollout，\r\n"
                },
                {
                    "date": 1671961121868,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -31,12 +31,11 @@\n \r\n ### 4.1 原始的（ALphaGo的）\r\n #### 4.1.1 MCTS部分\r\n 1. 蒙特卡洛：fastrollout\r\n-2. 树搜索：select-expand-backup\r\n+2. 树搜索UCT（UCB在树上的版本）：select-expand-backup\r\n \r\n-共四个步骤：select-expand-fastrollout-backup\r\n->UCT（UCB在树上的版本）\r\n+合起来共四个步骤：select-expand-fastrollout-backupS\r\n \r\n #### 4.1.2 神经网络部分\r\n 1. 首先使用模仿学习先监督学出一个弱鸡人类策略网络P_0，\r\n 2. 而后用P_0辅助fastrollout，\r\n"
                },
                {
                    "date": 1671961161804,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,10 +43,10 @@\n \r\n \r\n # 到Muzero，对比学习\r\n \r\n-看到Muzero和它的改进EfficientZero，发现同对比学习里的BYOL、Sim-Siam太像了。\r\n-也确实\r\n+看到Muzero和它的改进EfficientZero，发现efficientZero里第一个改进增加监督信号同对比学习里的BYOL、Sim-Siam太像了。\r\n+也确实。\r\n \r\n ## 5. MuZero ： 在state被embeding后的状态隐空间中MCTS\r\n 1. 只有在根节点处，muzero会排除规则不允许的行动。 而在之后的隐空间中mcts就没有限制了（“重要的是错误。”这意味着可以进行自监督的对比学习了）\r\n 2. 正式走子时一样，选择mcts搜索后第一层最大的N\r\n"
                },
                {
                    "date": 1671961180871,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,9 +47,9 @@\n 看到Muzero和它的改进EfficientZero，发现efficientZero里第一个改进增加监督信号同对比学习里的BYOL、Sim-Siam太像了。\r\n 也确实。\r\n \r\n ## 5. MuZero ： 在state被embeding后的状态隐空间中MCTS\r\n-1. 只有在根节点处，muzero会排除规则不允许的行动。 而在之后的隐空间中mcts就没有限制了（“重要的是错误。”这意味着可以进行自监督的对比学习了）\r\n+1. 只有在根节点处，muzero会排除规则不允许的行动。 而在之后的隐空间中mcts就没有限制了。\r\n 2. 正式走子时一样，选择mcts搜索后第一层最大的N\r\n 3. （人类使用图进行推理时，其实是在解码或“翻译”embeding到神经网络空间里的 算子）既然是在隐空间内plan，隐空间也可以再 投射（生成） 到graph的空间里\r\n 4. 这就打通了脑子了\r\n \r\n"
                },
                {
                    "date": 1671961248380,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,9 +50,10 @@\n ## 5. MuZero ： 在state被embeding后的状态隐空间中MCTS\r\n 1. 只有在根节点处，muzero会排除规则不允许的行动。 而在之后的隐空间中mcts就没有限制了。\r\n 2. 正式走子时一样，选择mcts搜索后第一层最大的N\r\n 3. （人类使用图进行推理时，其实是在解码或“翻译”embeding到神经网络空间里的 算子）既然是在隐空间内plan，隐空间也可以再 投射（生成） 到graph的空间里\r\n-4. 这就打通了脑子了\r\n+4. 这就打通了脑子了。投射谬误与沉没成本的联系，机制网络是投射谬误的结构。（要验证这句话，需要第一性。\"第一性\"就是\"若非\"）\r\n+​\r\n \r\n \r\n ## 6.对比学习(只要能定义正负例就可以自监督学习)\r\n 1. 那大脑其实也在做一样的事，增强现实（同时，也意味着，意识的虚假）\r\n"
                },
                {
                    "date": 1671961374747,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,14 +50,14 @@\n ## 5. MuZero ： 在state被embeding后的状态隐空间中MCTS\r\n 1. 只有在根节点处，muzero会排除规则不允许的行动。 而在之后的隐空间中mcts就没有限制了。\r\n 2. 正式走子时一样，选择mcts搜索后第一层最大的N\r\n 3. （人类使用图进行推理时，其实是在解码或“翻译”embeding到神经网络空间里的 算子）既然是在隐空间内plan，隐空间也可以再 投射（生成） 到graph的空间里\r\n-4. 这就打通了脑子了。投射谬误与沉没成本的联系，机制网络是投射谬误的结构。（要验证这句话，需要第一性。\"第一性\"就是\"若非\"）\r\n+4. 这就打通了脑子了。投射谬误与沉没成本的联系，机制网络是投射谬误的结构。（要验证这句话，需要第一性。所谓\"第一性\"就是\"若非\"）\r\n ​\r\n \r\n \r\n ## 6.对比学习(只要能定义正负例就可以自监督学习)\r\n 1. 那大脑其实也在做一样的事，增强现实（同时，也意味着，意识的虚假）\r\n 2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n-3. 错误提供信息，或者说，错误和规则等价\r\n+3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是穿起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。\r\n"
                },
                {
                    "date": 1671961383387,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,7 +57,7 @@\n \r\n ## 6.对比学习(只要能定义正负例就可以自监督学习)\r\n 1. 那大脑其实也在做一样的事，增强现实（同时，也意味着，意识的虚假）\r\n 2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n-3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题。\r\n+3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是穿起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。\r\n"
                },
                {
                    "date": 1671961408921,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -58,6 +58,6 @@\n ## 6.对比学习(只要能定义正负例就可以自监督学习)\r\n 1. 那大脑其实也在做一样的事，增强现实（同时，也意味着，意识的虚假）\r\n 2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n 3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n-4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是穿起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n+4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是串起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。\r\n"
                },
                {
                    "date": 1671961465551,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,5 +59,5 @@\n 1. 那大脑其实也在做一样的事，增强现实（同时，也意味着，意识的虚假）\r\n 2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n 3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是串起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n-5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。\r\n+5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint 回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n"
                },
                {
                    "date": 1671961471449,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,5 +59,5 @@\n 1. 那大脑其实也在做一样的事，增强现实（同时，也意味着，意识的虚假）\r\n 2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n 3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是串起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n-5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint 回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n+5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n"
                },
                {
                    "date": 1671961496900,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,4 +60,7 @@\n 2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n 3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是串起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n+6. 所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。\r\n+7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，就是机制网络，\r\n+​词向量空间就可以利用计算相似度，评估一些（什么东西）\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671961513229,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,6 +61,5 @@\n 3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是串起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n 6. 所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。\r\n-7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，就是机制网络，\r\n-​词向量空间就可以利用计算相似度，评估一些（什么东西）\n\\ No newline at end of file\n+7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671961551547,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,9 +54,9 @@\n 4. 这就打通了脑子了。投射谬误与沉没成本的联系，机制网络是投射谬误的结构。（要验证这句话，需要第一性。所谓\"第一性\"就是\"若非\"）\r\n ​\r\n \r\n \r\n-## 6.对比学习(只要能定义正负例就可以自监督学习)\r\n+## 6.对比学习(从只要能定义正负例就可以自监督学习)\r\n 1. 那大脑其实也在做一样的事，增强现实（同时，也意味着，意识的虚假）\r\n 2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n 3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是串起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n"
                },
                {
                    "date": 1671961646369,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,6 +60,6 @@\n 2. （对比学习的损失中，用align和uniform评价，有一部分是正比于在优化最大特征值别太离谱）\r\n 3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是串起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n-6. 所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。\r\n+6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671961832219,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,6 @@\n+[TOC]\r\n+\r\n # from_alphazero_back_to_bandits\r\n  从实现alphazero开始，回溯到Exploit-Explor问题\r\n >强化学习：bandits和alphazero\r\n \r\n"
                },
                {
                    "date": 1671961837992,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,5 +63,6 @@\n 3. reward shaping 可以认为是（对比学习中的）代理任务，它不是直接的奖励，所谓进行shaping要有的“先验”可以是显然的命题来自监督用。\r\n 4. \"这里学一点，那里学一点，那里在隐藏空间中相似可能有探索的价值。。。以为自己比别人多一个“自己的问题\"最后看起来像是串起来了，其实只是？？？把ee交换图里在lattin诱导的那个g学出来了\"\r\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n-7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\n\\ No newline at end of file\n+7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n+\r\n"
                },
                {
                    "date": 1671962086899,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,4 +65,5 @@\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n \r\n+# 这些字只是后来才理顺的。重要的是图\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671962095098,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,5 +65,5 @@\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n \r\n-# 这些字只是后来才理顺的。重要的是图\n\\ No newline at end of file\n+# 这些字只是后来才理顺的。重要的是建立联系了的图\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671962115873,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,5 +65,5 @@\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n \r\n-# 这些字只是后来才理顺的。重要的是建立联系了的图\n\\ No newline at end of file\n+# 这些字只是后来才理顺，打出来的。重要的是建立联系了的图\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671962130095,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,5 +65,5 @@\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n \r\n-# 这些字只是后来才理顺，打出来的。重要的是建立联系了的图\n\\ No newline at end of file\n+# 这些字的组织形式只是后来才理顺，打出来的，没啥用。重要的是建立联系了的图\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671962135267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,5 +65,5 @@\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n \r\n-# 这些字的组织形式只是后来才理顺，打出来的，没啥用。重要的是建立联系了的图\n\\ No newline at end of file\n+# 这些字的组织形式只是后来才理顺，打出来的，没啥用。重要的是之前过程中建立联系了的图\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671962378372,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,5 +65,6 @@\n 5. 我直觉的直觉中“理性只有一个功能，禁止（状态延续变化）\"。hint：回到脆弱的模样。指的是不是加强探索（可我直觉上要的似乎是某种泛化性，从不同材料得到相同共同的)\r\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n \r\n-# 这些字的组织形式只是后来才理顺，打出来的，没啥用。重要的是之前过程中建立联系了的图\n\\ No newline at end of file\n+# 这些字的组织形式只是后来才理顺，打出来的，没啥用。重要的是之前过程中建立联系了的图。\r\n+./sim_pics/\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671962386912,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,5 +66,5 @@\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n \r\n # 这些字的组织形式只是后来才理顺，打出来的，没啥用。重要的是之前过程中建立联系了的图。\r\n-./sim_pics/\n\\ No newline at end of file\n+> ./sim_pics/1-10\n\\ No newline at end of file\n"
                },
                {
                    "date": 1671962394301,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,5 +66,5 @@\n 6. 引用*所谓“基础好”，说白了是“能够把一切都很容易的变成自己的东西”；走到了这个良性循环之内，才能让人“学的越多、知道的越少”——知道的越少，那么把新东西纳入体系才更不费力。就好像你用“并行计算”轻易把map-reduce串进去一样。*\r\n 7. ​两个状态之间的动作（动作也可以embedding抽象成文字描述)，​词向量空间就可以利用计算相似度，评估一些（什么东西）\r\n \r\n # 这些字的组织形式只是后来才理顺，打出来的，没啥用。重要的是之前过程中建立联系了的图。\r\n-> ./sim_pics/1-10\n\\ No newline at end of file\n+> ./sim_pics/1~10\n\\ No newline at end of file\n"
                }
            ],
            "date": 1669278523511,
            "name": "Commit-0",
            "content": "# from_alphazero_back_to_bandits\n 从实现alphazero开始，回溯到Exploit-Explor问题\n>强化学习：bandits和alphazero\n\n## 1.“通用（最小化总遗憾的策略）过程比结果重要”\n\n>问题：如果做（Exploit-Explore trade off）算力聚焦的时候，有一片区域在你的探索过程压根就探索不到的情况下\n\n## 2.选择是一个技术活。专治选择困难症——bandits算法\n\n什么策略才能最大化得到的奖励（最小化N步时的累计遗憾）？通过探索（explore）找到最佳分布的老虎机，并且以一种快速而高效的方法找到这个老虎机，且不断的利用（exploit）它获得最大化的奖励。\n\n1. Thompson sampling算法：每个臂维护一个beta分布的参数，每次试验后，选中一个臂，拉一下，有收益则该臂的wins+=1，否则该臂的loss+=1.每次臂的选择方式是：用每个臂现有的beta分布产生一个随机数x，选择所有臂产生的随机数中最大的那个臂去拉。\n2. UCB（Upper Confidence Bound）算法：均值±变差 (V_i/N_i + (k*ln(N_total)/N_i)^0.5)\n3. Epsilon-Greedy算法：每次以概率epsilon∈（0,1）做一件事，所有壁中随机选一个拉，否则，选择截止当前平均收益最大的那个臂拉一下。（epsilon的值可以控制对E-E trade off ，越接近0，越保守）\n4. 完全朴素（极大似然）：先试验一些次，每个臂都有均值后，一直选择均值最大的那个臂，这个算法是人类在实际中最常采用的。\n\n>算法效果对比一目了然：UCB和Thonmpson采样算法显著优秀。至于你实际上要选哪一种bandits算法？你可以选一种bandits算法来选bandits算法。\n\n## 3.用bandits算法解决推荐系统冷启动的简单思路\n\n我想，屏幕前的你已经想到了，推荐系统冷启动可以用bandits算法来解决一部分。\n>LinUCB算法：单纯的老虎机，他回报情况就是老虎机自己内部决定的，而在广告推荐领域，一个选择的回报，是由state_i（用户状态）和action_i一起决定的，如果我们能用feature来刻画state_i和action_i这一对儿（是不是到这儿才品出来有RL的味了。。），在选择之前通过feature预估每一个arm的期望回报及置信区间，那就合理多了。\n\n利用UCB（最乐观选择，SCB是最悲观）来调整E-E trade off，节点探索的次数越多，置信区间越窄；每次选择置信度区间上界最大的节点max（feature_i + (k*ln(N)/1+Tn_i)^0.5）来平衡探索的次数和未探索的节点的。\n\n\n### 4. 总的来说，AlphaGo Zero分为两个部分，一部分是MCTS（蒙特卡洛树搜索），一部分是神经网络。\n\n我们是要抛弃人类棋谱的，学会如何下棋完全是通过自对弈来完成。\n\n过程是这样，首先生成棋谱，然后将棋谱作为输入训练神经网络，训练好的神经网络用来预测落子和胜率。\n\nMCTS就是用来自对弈生成棋谱的，\n>a. 每次模拟通过选择具有最大行动价值Q的边加上取决于所存储的先验概率P和该边的访问计数N（每次访问都被增加一次）的上限置信区间U来遍历树。\nb. 展开叶子节点，通过神经网络来评估局面s；向量P的值存储在叶子结点扩展的边上。\nc. 更新行动价值Q等于在该行动下的子树中的所有评估值V的均值。\nd. 一旦MCTS搜索完成，返回局面s下的落子概率π，与N^(1/τ)成正比，其中N是从根状态每次移动的访问计数， τ是控制温度的参数。\n\n按照论文所述，每次MCTS使用1600次模拟。过程是这样的，现在AI从白板一块开始自己跟自己下棋，只知道规则，不知道套路，那只好乱下。每下一步棋，都要通过MCTS模拟1600次上图中的a~c，从而得出我这次要怎么走子。\n来说说a~c，MCTS本质上是我们来维护一棵树，这棵树的每个节点保存了每一个局面（situation）该如何走子（action）的信息。这些信息是，N(s, a)是访问次数，W(s, a)是总行动价值，Q(s, a)是平均行动价值，P(s, a)是被选择的概率。\n\n##### 4.1.1 a.Select\n每次模拟的过程都一样，从父节点的局面开始，选择一个走子。比如开局的时候，所有合法的走子都是可能的选择，那么我该选哪个走子呢？这就是select要做的事情。MCTS选择Q(s, a) + U(s, a)最大的那个action。\n\n- U(s, a) = c_puct × 概率P(s, a) × np.sqrt(父节点访问次数N) / ( 1 + 某子节点action的访问次数N(s, a) )\n- feature_i = Q(s_t,a)   ：把当前的局面作为输入传给神经网络，神经网络会返回给我们一个action向量p和当前胜率v。\n\n用论文中的话说，c_puct是一个决定探索水平的常数；这种搜索控制策略最初倾向于具有高先验概率和低访问次数的行为，但是渐近地倾向于具有高行动价值的行为。\n计算过后，我就知道当前局面下，哪个action的Q+U值最大，那这个action走子之后的局面就是第二次模拟的当前局面。比如开局，Q+U最大的是当头炮，然后我就Select当头炮这个action，再下一次Select就从当头炮的这个棋局选择下一个走子。\n\n##### 4.1.2 b.Expand\n现在开始第二次模拟了，假如之前的action是当头炮，我们要接着这个局面选择action，但是这个局面是个叶子节点。就是说当头炮之后可以选择哪些action不知道，这样就需要expand了，通过expand得到一系列可能的action节点。这样实际上就是在扩展这棵树，从只有根节点开始，一点一点的扩展。\n\n##### 4.1.3 c.Evaluate（fast rollout）\n我们使用虚拟损失来确保每个线程评估不同的节点。\n因为我们是多线程同时在做MCTS，由于Select算法都一样，都是选择Q+U最大节点，所以很有可能所有的线程最终选择的是同一个节点，这就尴尬了。我们的目的是尽可能在树上搜索出各种不同的着法，最终选择一步好棋，怎么办呢？论文中已经给出了办法，“我们使用虚拟损失来确保每个线程评估不同的节点。”就是说，通过Select选出某节点后，人为增大这个节点的访问次数N，并减少节点的总行动价值W，因为平均行动价值Q = W / N，这样分子减少，分母增加，就减少了Q值，这样递归进行的时候，此节点的Q+U不是最大，避免被选中，让其他的线程尝试选择别的节点进行树搜索。这个人为增加和减少的量就是虚拟损失virtual loss。\n\n##### 4.1.4 c.Backup\n现在MCTS的过程越来越清晰了，Select选择节点，选择后，对当前节点使用虚拟损失，通过递归继续Select，直到分出胜负或Expand节点，得到返回值value。现在就可以使用value进行Backup了，但首先要还原W和N，之前N增加了虚拟损失，这次要减回去，之前减少了虚拟损失的W也要加回来。然后开始做Backup.\n\n\n#### d. Play\n按照上述过程执行a~c，论文中是每步棋执行1600次模拟，那就是1600次的a~c，这个MCTS的过程就是模拟自我对弈的过程。模拟结束后，基本上能覆盖大多数的棋局和着法，每步棋该怎么下，下完以后胜率是多少，得到什么样的局面都能在树上找到。然后从树上选择当前局面应该下哪一步棋，这就是步骤d.play:\"在搜索结束时，AlphaGo Zero在根节点s0选择一个走子a，与其访问计数幂指数成正比.\n\n在随后的时间步重新使用搜索树：与所走子的动作对应的子节点成为新的根节点；保留这个节点下面的子树所有的统计信息，而树的其余部分被丢弃。\n\n当模拟结束后，对于当前局面（就是树的根节点）的所有子节点就是每一步对应的action节点，选择哪一个action呢？按照论文所说是通过访问计数N来确定的。这个好理解吧？实现上也容易，当前节点的所有节点是可以获得的，每个子节点的信息N都可以获得，然后从多个action中选一个\n\n#### NerosNetwork\n上面说过，通过MCTS算出该下哪一步棋。然后接着再经过1600次模拟算出下一步棋，如此循环直到分出胜负，这样一整盘棋就下完了，这就是一次完整的自对弈过程，那么MCTS就相当于人在大脑中思考。我们把每步棋的局面s_t 、算出的action概率向量 π_t 和胜率z_t （就是返回值value）保存下来，作为棋谱数据训练神经网络。\n\n训练目标是最小化预测胜率v和自我对弈的胜率z之间的误差，并使神经网络走子概率p与搜索概率π的相似度最大化。简单点说就是让神经网络的预测跟MCTS的搜索结果尽量接近。\n\nMCTS和神经网络你中有我、我中有你，如此反复迭代，网络预测的更准确，MCTS的结果更强大。在AlphaGo Zero中，自我对弈是由以前所有迭代中最好的玩家生成的。每次训练迭代之后，与最好玩家对弈测量新玩家的能力；如果以55%的优势获胜，那么它将取代最好的玩家，而自我对弈将由这个新玩家产生。"
        }
    ]
}