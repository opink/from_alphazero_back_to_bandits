{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用五字棋尝试一下\n",
    "# You can change this to another two-player game.\n",
    "# 给状态张量增加一个channel表示当前行棋方\n",
    "\n",
    "# TODO 1.实现 train Nets on GPU. Done on 2023.1.7\n",
    "# TODO 2.implement a memoryband storing trails for training network. Done on 2023.1.11\n",
    "# TODO 3.实现训练sudoindex（比如轨迹长度的分布等等）收集至wandb. Done on 2023.1.19\n",
    "\n",
    "\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "\n",
    "BLACK, WHITE = 1, -1  # 颜色的先后手,黑圈O先,白叉X后\n",
    "\n",
    "class State:\n",
    "    '''实现 15 x 15 wuziqi 的棋盘'''\n",
    "    X, Y = 'ABCDEFGHIJKLMNO',  '123456789uvwxyz'\n",
    "    C = {0: '_', BLACK: 'O', WHITE: 'X'}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((15, 15)) # (x, y)\n",
    "        self.color = 1\n",
    "        self.win_color = 0\n",
    "        self.record = []\n",
    "\n",
    "    def action2str(self, a:int):\n",
    "        \"\"\"用0-224编码落子位\n",
    "            '1'  '2'  '3'   ... 'z'\n",
    "        'A'  0    1    2    ...  14\n",
    "        'B'  15   16   17   ...  29\n",
    "        'C'  30   31   32   ...  44\n",
    "        ...                 ...\n",
    "        'O'  210  211  212  ...  224\n",
    "        \"\"\"\n",
    "        return self.X[a // 15] + self.Y[a % 15]\n",
    "\n",
    "    def str2action(self, s:str):\n",
    "        return self.X.find(s[0]) * 15 + self.Y.find(s[1])\n",
    "\n",
    "    def record_string(self):\n",
    "        \"\"\"记录动作的列表组装成字符串序列，用空格分隔\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            一条动作的（轨迹）字符串，空格分隔方便split\n",
    "        \"\"\"\n",
    "        return ' '.join([self.action2str(a) for a in self.record])\n",
    "\n",
    "    def __str__(self):\n",
    "        # 打印棋盘\n",
    "        s = '   ' + ' '.join(self.Y) + '\\n'\n",
    "        for i in range(15):\n",
    "            s += self.X[i] + ' ' + ' '.join([self.C[self.board[i, j]] for j in range(15)]) + '\\n'\n",
    "        s += 'record = ' + self.record_string()\n",
    "        return s\n",
    "\n",
    "    def check_win(self, x:int, y:int):\n",
    "        # check whether 5 stones are on the line , pad zero around board by (4,4) then compute if sum to 5*self.color\n",
    "        x_tmp, y_tmp = x + 4, y + 4\n",
    "        boardex4 = np.pad(self.board,(4,4))\n",
    "        for i in range(5):\n",
    "            if sum(boardex4[x_tmp-4+i:x_tmp+1+i, y_tmp])==5*self.color:\n",
    "                return True\n",
    "            elif sum(boardex4[x_tmp, y_tmp-4+i:y_tmp+1+i])==5*self.color:\n",
    "                return True\n",
    "            elif boardex4[x_tmp+i-4,y_tmp+i-4]+boardex4[x_tmp+i-3,y_tmp+i-3]+boardex4[x_tmp+i-2,y_tmp+i-2]+\\\n",
    "                    boardex4[x_tmp+i-1,y_tmp+i-1]+boardex4[x_tmp+i,y_tmp+i]==5*self.color:\n",
    "                return True\n",
    "            elif boardex4[x_tmp+i-4,y_tmp-i+4]+boardex4[x_tmp+i-3,y_tmp-i+3]+boardex4[x_tmp+i-2,y_tmp-i+2]+\\\n",
    "                    boardex4[x_tmp+i-1,y_tmp-i+1]+boardex4[x_tmp+i,y_tmp-i]==5*self.color:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def play(self, action:Union[str, int]) -> 'State':\n",
    "        # 关于type hint : Python中的类是在读取完整个类之后才被定义的，因此在类中无法通过正常方式表示这个类本身。\n",
    "        # 替代方法是使用一个和类同名的字符串，这被称为自引用类型。\n",
    "        \"\"\"状态转移\n",
    "        Parameters\n",
    "        ----------\n",
    "            action : 0-80的落子位置int,或者动作的用空格分隔的字符串序列str\n",
    "        Returns\n",
    "        -------\n",
    "            self\n",
    "        \"\"\"\n",
    "        # 如果是一条（轨迹）字符串（该对象对应有数据结构的设计，一定程度上组织成有可遍历的特征）\n",
    "        # 那么，可直接设计递归的调用，归约为仅需实现单次的int输入的动作状态转移\n",
    "        if isinstance(action, str):\n",
    "            for astr in action.split(): # 默认用空格分隔字符串\n",
    "                self.play(self.str2action(astr))\n",
    "            return self\n",
    "\n",
    "        x, y = action // 15, action % 15\n",
    "        self.board[x, y] = self.color\n",
    "\n",
    "        # # 检查是否5子连线\n",
    "        if self.check_win(x , y):\n",
    "            self.win_color = self.color\n",
    "\n",
    "        self.color = -self.color\n",
    "        self.record.append(action)\n",
    "        return self\n",
    "\n",
    "    def terminal(self):\n",
    "        # 终止状态检查，用于selfplay循环条件\n",
    "        return self.win_color != 0 or len(self.record) == 15 * 15\n",
    "\n",
    "    def terminal_reward(self):\n",
    "        # 返回终局奖励 1，-1\n",
    "        return self.win_color if self.color == BLACK else -self.win_color\n",
    "\n",
    "    def legal_actions(self) -> List[int]:\n",
    "        # 返回根节点下的合法走子位，List of int\n",
    "        return [a for a in range(15 * 15) if self.board[a // 15, a % 15] == 0]\n",
    "\n",
    "    def feature(self, to_cuda:bool = False):\n",
    "        # making input ndarray for NN_state\n",
    "        # 堆个ndarry用作神经网络输入 : [当前行动者 ,我方视角棋盘，对方视角棋盘]\n",
    "        # support sending ndarry to cuda tensor with added a batch_dim\n",
    "        now_mover = np.ones((15, 15)) * self.color # 加1通道进卷积\n",
    "        s = np.stack([now_mover, self.board == self.color, self.board == -self.color]).astype(np.float32)\n",
    "        if to_cuda:\n",
    "            return torch.from_numpy(s).unsqueeze(0).cuda()\n",
    "        return s\n",
    "\n",
    "    def action_feature(self, action, to_cuda:bool = False):\n",
    "        # 制作动作矩阵\n",
    "        # support sending ndarry to cuda tensor with added a batch_dim\n",
    "        a = np.zeros((1, 15, 15), dtype=np.float32)\n",
    "        a[0, action // 15, action % 15] = 1\n",
    "        if to_cuda:\n",
    "            return torch.from_numpy(a).unsqueeze(0).cuda()\n",
    "        return a\n",
    "\n",
    "# state = State().play('A2')\n",
    "# print(state)\n",
    "# print('input feature')\n",
    "# print(state.feature())\n",
    "# state = State().play('B2 A1 I2')\n",
    "# print(state)\n",
    "# print('input feature')\n",
    "# print(state.feature())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义组件网络Res&Conv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, filters0, filters1, kernel_size, bn=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn = None\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm2d(filters1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            h = self.bn(h)\n",
    "        return h\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(filters, filters, 3, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + (self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 64\n",
    "num_blocks = 12\n",
    "\n",
    "class Representation(nn.Module):\n",
    "    ''' Conversion from observation to inner abstract state '''\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape # (c, 15, 15)\n",
    "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
    "        # 初始化nn.Conv2d inputchannels，outputchannels\n",
    "        self.layer0 = Conv(self.input_shape[0], num_filters, 3, bn=True)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.layer0(x))\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h # torch.Size([1, 16, 15, 15])\n",
    "\n",
    "    def inference(self, x, pass_to_cpu:bool = True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # rp = self(torch.from_numpy(x).unsqueeze(0)) # cpu_only版本: conv2d的输入tensor需要四维，多加一维度在0位置\n",
    "            rp = self(x)\n",
    "        if not pass_to_cpu:\n",
    "            return rp # print('rp tensor shape' , rp.shape) # torch.Size([1, 16, 15, 15])\n",
    "        return rp.cpu().numpy()[0]  \n",
    "\n",
    "class Prediction(nn.Module):\n",
    "    ''' Policy and value prediction from inner abstract state '''\n",
    "    def __init__(self, action_shape):\n",
    "        super().__init__()\n",
    "        self.board_size = np.prod(action_shape[1:]) # 15 x 15 = 81\n",
    "        self.action_size = action_shape[0] * self.board_size # 1 x 81 = 81\n",
    "\n",
    "        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)\n",
    "        self.conv_p2 = Conv(4, 1, 1)\n",
    "\n",
    "        self.conv_v = Conv(num_filters, 4, 1, bn=True)\n",
    "        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)\n",
    "\n",
    "    def forward(self, rp):\n",
    "        h_p = F.relu(self.conv_p1(rp))\n",
    "        # print('过第一层p卷积', h_p.shape) # torch.Size([1, 4, 15, 15])\n",
    "        h_p = self.conv_p2(h_p).view(-1, self.action_size)\n",
    "        # print('过第二层p卷积', h_p.shape) # torch.Size([1, 81])\n",
    "        h_v = F.relu(self.conv_v(rp))\n",
    "        # print('过第一层fc卷积', h_v.shape) # torch.Size([1, 4, 15, 15])\n",
    "        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))\n",
    "        # print('过第二层fc层', h_v.shape) # torch.Size([1, 1])\n",
    "        # range of value is -1 ~ 1\n",
    "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
    "\n",
    "    def inference(self, rp, pass_to_cpu:bool = True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # p, v = self(torch.from_numpy(rp).unsqueeze(0))\n",
    "            p, v = self(rp) # print('p shape is ', p.shape) # torch.Size([1, 81])\n",
    "        if not pass_to_cpu:\n",
    "            return p, v\n",
    "        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "    '''Abstract state transition'''\n",
    "    def __init__(self, rp_shape, act_shape):\n",
    "        super().__init__()\n",
    "        self.rp_shape = rp_shape\n",
    "        self.layer0 = Conv(rp_shape[0] + act_shape[0], num_filters, 3, bn=True)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, rp, a):\n",
    "        h = torch.cat([rp, a], dim=1)\n",
    "        # print('dim=1 cat shape h ' , h.shape) # torch.Size([1, 17, 15, 15])\n",
    "        h = self.layer0(h)\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, rp, a, pass_to_cpu:bool = True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # cpu_only版本: rp = self(torch.from_numpy(rp).unsqueeze(0), torch.from_numpy(a).unsqueeze(0))\n",
    "            rp = self(rp, a)\n",
    "        if not pass_to_cpu:\n",
    "            return rp\n",
    "        return rp.cpu().numpy()[0]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''Whole net'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        state = State()\n",
    "        input_shape = state.feature().shape # state (c, 15, 15)\n",
    "        action_shape = state.action_feature(0).shape # action (1, 15, 15)\n",
    "        rp_shape = (num_filters, *input_shape[1:]) # hidden space (16, 15, 15)\n",
    "\n",
    "        self.representation = Representation(input_shape)\n",
    "        self.prediction = Prediction(action_shape)\n",
    "        self.dynamics = Dynamics(rp_shape, action_shape)\n",
    "\n",
    "    def predict(self, state0, path):\n",
    "        '''Predict p and v from original state and path'''\n",
    "        outputs = []\n",
    "        x = state0.feature(to_cuda=True)\n",
    "        rp = self.representation.inference(x, pass_to_cpu= False)\n",
    "        outputs.append(self.prediction.inference(rp, pass_to_cpu = True))\n",
    "        for action in path:\n",
    "            a = state0.action_feature(action, to_cuda=True)\n",
    "            rp = self.dynamics.inference(rp, a, pass_to_cpu = False)\n",
    "            outputs.append(self.prediction.inference(rp, pass_to_cpu = True))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 2 3 4 5 6 7 8 9 u v w x y z\n",
      "A _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "B _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "D _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "E _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "F _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "G _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "H _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "I _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "J _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "K _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "L _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "M _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "N _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "record = \n",
      "p = \n",
      "[[[45 43 45 45 46 46 46 46 46 46 46 46 45 46 43]\n",
      "  [46 45 44 46 46 46 46 46 46 46 46 46 46 46 45]\n",
      "  [45 42 41 44 45 45 45 45 45 45 45 45 45 45 46]\n",
      "  [44 40 40 43 44 44 44 44 44 44 44 45 45 45 45]\n",
      "  [44 40 40 43 44 44 44 44 44 44 44 45 45 45 45]\n",
      "  [44 40 40 43 44 44 44 44 44 44 44 46 45 45 45]\n",
      "  [44 40 41 43 44 44 45 44 44 45 45 46 45 45 45]\n",
      "  [44 41 41 43 44 44 45 45 45 45 45 46 45 45 45]\n",
      "  [44 41 41 43 44 44 45 44 44 45 45 46 45 45 45]\n",
      "  [44 40 40 43 44 44 44 44 44 44 44 46 45 45 45]\n",
      "  [44 40 41 43 44 44 44 44 44 44 44 46 45 45 45]\n",
      "  [44 40 41 43 44 44 44 44 44 44 44 45 45 45 45]\n",
      "  [43 40 41 43 45 45 45 45 45 45 45 45 45 45 45]\n",
      "  [43 39 41 41 42 42 42 42 42 42 42 42 41 45 43]\n",
      "  [44 42 42 42 43 43 43 43 43 43 43 43 42 44 44]]]\n",
      "v =  0.29715642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 给上面的三个网络做单元测试用\n",
    "def show_net(net, state):\n",
    "    '''Display policy (p) and value (v)'''\n",
    "    print(state)\n",
    "    p, v = net.predict(state, [])[-1]\n",
    "    print('p = ')\n",
    "    print((p * 10000).astype(int).reshape((-1, *net.representation.input_shape[1:3])))\n",
    "    print('v = ', v)\n",
    "    print()\n",
    "\n",
    "#  Outputs before training\n",
    "show_net(Net().cuda(), State())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现蒙特卡洛树搜索MCTS\n",
    "\n",
    "class Node:\n",
    "    '''Search result of one abstract (or root) state'''\n",
    "    def __init__(self, p, v):\n",
    "        self.p, self.v = p, v\n",
    "        self.n, self.q_sum = np.zeros_like(p), np.zeros_like(p)\n",
    "        self.n_all, self.q_sum_all = 1, v / 2 # prior\n",
    "\n",
    "    def update(self, action, q_new):\n",
    "        # Update\n",
    "        self.n[action] += 1\n",
    "        self.q_sum[action] += q_new\n",
    "\n",
    "        # Update overall stats\n",
    "        self.n_all += 1\n",
    "        self.q_sum_all += q_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "class Tree:\n",
    "    '''Monte Carlo Tree'''\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "        self.nodes = {}\n",
    "\n",
    "    def search(self, state, path, rp, depth):\n",
    "        # Return predicted value from new state\n",
    "        # rp is a Tensor on Gpu\n",
    "        key = state.record_string()\n",
    "        if len(path) > 0:\n",
    "            key += '|' + ' '.join(map(state.action2str, path))\n",
    "        if key not in self.nodes:\n",
    "            p, v = self.net.prediction.inference(rp, pass_to_cpu = True)\n",
    "            self.nodes[key] = Node(p, v)\n",
    "            return v\n",
    "\n",
    "        # State transition by an action selected from bandit\n",
    "        node = self.nodes[key]\n",
    "        p = node.p\n",
    "        mask = np.zeros_like(p)\n",
    "        if depth == 0:\n",
    "            # Add noise to policy on the root node\n",
    "            p = 0.75 * p + 0.25 * np.random.dirichlet([0.15] * len(p))\n",
    "            # On the root node, we choose action only from legal actions\n",
    "            mask[state.legal_actions()] = 1\n",
    "            p *= mask\n",
    "            p /= p.sum() + 1e-16\n",
    "\n",
    "        n, q_sum = 1 + node.n, node.q_sum_all / node.n_all + node.q_sum\n",
    "        ucb = q_sum / n + 2.0 * np.sqrt(node.n_all) * p / n + mask * 4 # PUCB formula\n",
    "        best_action = np.argmax(ucb)\n",
    "\n",
    "        # Search next state by recursively calling this function\n",
    "        rp_next = self.net.dynamics.inference(rp, state.action_feature(best_action, to_cuda=True), pass_to_cpu=False)\n",
    "        path.append(best_action)\n",
    "        q_new = -self.search(state, path, rp_next, depth + 1) # With the assumption of changing player by turn\n",
    "        node.update(best_action, q_new)\n",
    "\n",
    "        return q_new\n",
    "\n",
    "    def think(self, state, num_simulations, temperature = 0, show=False):\n",
    "        # End point of MCTS\n",
    "        if show:\n",
    "            print(state)\n",
    "        start, prev_time = time.time(), 0\n",
    "        project_once = self.net.representation.inference(state.feature(to_cuda=True), pass_to_cpu=False) # Muzero在根节点状态只需rp一次，因为rpnet是固定的\n",
    "        for _ in range(num_simulations):\n",
    "            self.search(state, [], project_once, depth=0)\n",
    "            # Display search result on every second\n",
    "            if show:\n",
    "                tmp_time = time.time() - start\n",
    "                if int(tmp_time) > int(prev_time):\n",
    "                    prev_time = tmp_time\n",
    "                    root, pv = self.nodes[state.record_string()], self.pv(state)\n",
    "                    print('%.2f sec. best %s. q = %.4f. n = %d / %d. pv = %s'\n",
    "                          % (tmp_time, state.action2str(pv[0]), root.q_sum[pv[0]] / root.n[pv[0]],\n",
    "                             root.n[pv[0]], root.n_all, ' '.join([state.action2str(a) for a in pv])))\n",
    "\n",
    "        #  Return probability distribution weighted by the number of simulations\n",
    "        root = self.nodes[state.record_string()]\n",
    "        n = root.n + 1\n",
    "        n = (n / np.max(n)) ** (1 / (temperature + 1e-8))\n",
    "        return n / n.sum() # teacher--MCTS\n",
    "\n",
    "    def pv(self, state):\n",
    "        # Return principal variation (action sequence which is considered as the best)\n",
    "        s, pv_seq = copy.deepcopy(state), []\n",
    "        while True:\n",
    "            key = s.record_string()\n",
    "            if key not in self.nodes or self.nodes[key].n.sum() == 0:\n",
    "                break\n",
    "            best_action = sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1])[0][0]\n",
    "            pv_seq.append(best_action)\n",
    "            s.play(best_action)\n",
    "        return pv_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 2 3 4 5 6 7 8 9 u v w x y z\n",
      "A _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "B _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "D _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "E _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "F _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "G _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "H _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "I _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "J _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "K _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "L _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "M _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "N _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "record = \n",
      "   1 2 3 4 5 6 7 8 9 u v w x y z\n",
      "A _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "B _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "D _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "E _ _ _ O O O O _ _ _ _ _ _ _ _\n",
      "F _ _ _ _ X X X _ _ _ _ _ _ _ _\n",
      "G _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "H _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "I _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "J _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "K _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "L _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "M _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "N _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "record = E4 F5 E5 F6 E6 F7 E7\n",
      "[[0.0625 0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.     0.0625 0.     0.     0.0625 0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.0625 0.     0.    ]\n",
      " [0.     0.     0.     0.     0.     0.     0.     0.0625 0.     0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.     0.     0.     0.     0.0625 0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.     0.0625 0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.0625 0.     0.     0.     0.     0.     0.0625 0.     0.     0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.0625 0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.0625]\n",
      " [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.0625 0.     0.     0.     0.    ]\n",
      " [0.     0.     0.0625 0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.    ]\n",
      " [0.0625 0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.0625]\n",
      " [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.0625 0.     0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "# Search with initialized net\n",
    "\n",
    "tree = Tree(Net().cuda())\n",
    "next_step_0 = tree.think(State(), 100, show=True)\n",
    "\n",
    "tree = Tree(Net().cuda())\n",
    "next_step_n = tree.think(State().play('E4 F5 E5 F6 E6 F7 E7'), 200, show=True)\n",
    "print(next_step_n.reshape((15, 15)))\n",
    "\n",
    "# tree = Tree(Net().cuda())\n",
    "# tree.think(State().play('F4 D5 F5 D6 F6 D7 F7'), 200, show=True)\n",
    "\n",
    "# tree = Tree(Net().cuda())\n",
    "# tree.think(State().play('B2 A2 A3 C1'), 200, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of neural net\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "batch_size = 512\n",
    "num_steps = 100\n",
    "\n",
    "def gen_target(ep, k):\n",
    "    '''Generate inputs and targets for training'''\n",
    "    # path, reward, observation, action, policy\n",
    "    turn_idx = np.random.randint(len(ep[0]))\n",
    "    ps, vs, ax = [], [], []\n",
    "    for t in range(turn_idx, turn_idx + k + 1):\n",
    "        if t < len(ep[0]):\n",
    "            p = ep[4][t]\n",
    "            a = ep[3][t]\n",
    "        else: # state after finishing game\n",
    "            # p is 0 (loss is 0)\n",
    "            p = np.zeros_like(ep[4][-1])\n",
    "            # random action selection\n",
    "            a = np.zeros(np.prod(ep[3][-1].shape), dtype=np.float32)\n",
    "            a[np.random.randint(len(a))] = 1\n",
    "            a = a.reshape(ep[3][-1].shape)\n",
    "        vs.append([ep[1] if t % 2 == 0 else -ep[1]])\n",
    "        ps.append(p)\n",
    "        ax.append(a)\n",
    "        \n",
    "    return ep[2][turn_idx], ax, ps, vs\n",
    "\n",
    "def train(episodes, net, optimizer):\n",
    "    #     episodes = List(record:List[int], \n",
    "    #                     reward:int(0,1,-1), \n",
    "    #                     features:state.feature(), \n",
    "    #                     action_features:state.action_feature(action) from random.choice based on distribution p_targets, \n",
    "    #                     p_targets:teacher--MCTS))\n",
    "    '''Train neural net on GPU'''\n",
    "    p_loss_sum, v_loss_sum = torch.as_tensor(0, dtype=torch.float32).cuda() , torch.as_tensor(0, dtype=torch.float32).cuda()\n",
    "    net.train()\n",
    "    k = 3 # 与alpha不同（2个状态对就可以），至少3个连续状态用来训练Muzero\n",
    "    for _ in tqdm(range(num_steps)):\n",
    "        x, ax, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))], k) for j in range(batch_size)])\n",
    "        x = torch.from_numpy(np.array(x)).cuda()\n",
    "        ax = torch.from_numpy(np.array(ax))\n",
    "        p_target = torch.from_numpy(np.array(p_target))\n",
    "        v_target = torch.FloatTensor(np.array(v_target))\n",
    "\n",
    "        # Change the order of axis as [time step, batch, ...]\n",
    "        ax = torch.transpose(ax, 0, 1).cuda()\n",
    "        p_target = torch.transpose(p_target, 0, 1).cuda()\n",
    "        v_target = torch.transpose(v_target, 0, 1).cuda()\n",
    "\n",
    "        # Compute losses for k (+ current) steps\n",
    "        p_loss, v_loss = torch.as_tensor(0, dtype=torch.float32).cuda() , torch.as_tensor(0, dtype=torch.float32).cuda()\n",
    "        for t in range(k + 1):\n",
    "            rp = net.representation(x) if t == 0 else net.dynamics(rp, ax[t - 1])\n",
    "            p, v = net.prediction(rp)\n",
    "            p_loss += F.kl_div(torch.log(p), p_target[t], reduction='sum')\n",
    "            v_loss += torch.sum(((v_target[t] - v) ** 2) / 2)\n",
    "\n",
    "        p_loss_sum += p_loss.item()\n",
    "        v_loss_sum += v_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (p_loss + v_loss).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    num_train_datum = num_steps * batch_size\n",
    "    print('p_loss %f v_loss %f' % (p_loss_sum.cpu().numpy() / num_train_datum, v_loss_sum.cpu().numpy() / num_train_datum))\n",
    "    wandb.log({\n",
    "        'p-loss': p_loss_sum.cpu().numpy() / num_train_datum,\n",
    "        'v-loss': v_loss_sum.cpu().numpy() / num_train_datum})\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Battle against random agents\n",
    "import plotly_express as px\n",
    "\n",
    "def vs_random(net, n=100):\n",
    "    results = { 0 : 0 , -1 : 0 , 1 : 0}\n",
    "    length_of_turn = []\n",
    "    for i in range(n):\n",
    "        first_turn = i % 2 == 0\n",
    "        turn = first_turn\n",
    "        state = State()\n",
    "        steps = 0\n",
    "        while not state.terminal():\n",
    "            if turn:\n",
    "                p, _ = net.predict(state, [])[-1]\n",
    "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\n",
    "            else:\n",
    "                action = np.random.choice(state.legal_actions())\n",
    "\n",
    "            state.play(action)\n",
    "            steps += 1\n",
    "            turn = not turn\n",
    "        length_of_turn.append(steps)\n",
    "        r = state.terminal_reward() if turn else -state.terminal_reward()\n",
    "        results[r] = results.get(r, 0) + 1\n",
    "    # 记录棋局长度的提琴图，方便之后继续训练比较使用\n",
    "    plt = px.violin(length_of_turn, box=True, points= 'all')\n",
    "    wandb.log({'length of turns vs. randomplayer': plt})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B1', 'N1', 'Bz', 'A2', 'Oy', 'O2', 'Nz', 'Ay']\n"
     ]
    }
   ],
   "source": [
    "# enrich self-play episodes from symmetry\n",
    "\n",
    "def symmetrys(action:int) -> List[int]:\n",
    "    \"\"\"15x15的黑白棋盘一共8种对称性，因此1条self-play轨迹可以扩充7条数据\n",
    "    Parameters\n",
    "    ----------\n",
    "    action : int\n",
    "        每一步执行的动作\n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        [identify]id0, [上下翻转]id1, [左右翻转]id2, [左斜轴翻转]id3, [右斜轴翻转]id4, [向左旋转π/2]id5, [向左旋转π]id6, [向右旋转π/2]id7\n",
    "    \"\"\"\n",
    "    x, y = action // 15, action % 15\n",
    "    id1 = (14 - x) * 15 + y\n",
    "    id2 = x * 15        + (14 - y)\n",
    "    id3 = y * 15        + x\n",
    "    id4 = (14 - y) * 15 + (14 - x)\n",
    "    id5 = (14 - y) * 15 + x\n",
    "    id6 = 224-action\n",
    "    x7 , y7 = id6 // 15, id6 % 15\n",
    "    id7 = (14 - y7) * 15 + x7\n",
    "    return [action, id1, id2, id3, id4, id5, id6, id7]\n",
    "\n",
    "print([State().action2str(i) for i in symmetrys(15)]) #　checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'202301191444'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# steps = [[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]\n",
    "# for idx in range(4):\n",
    "#     print([step[idx] for step in steps])\n",
    "\n",
    "# test保存路径\n",
    "import time\n",
    "'.model/checkpoints/{}/'.format(time.strftime('%Y%m%d%H%M')).split('/')[-2]\n",
    "# checked\n",
    "\n",
    "\n",
    "# test保存deque\n",
    "# import pickle\n",
    "# episodes = [1]\n",
    "# with open('.model/checkpoints/' + 'episodes.pt', 'wb') as ep:\n",
    "#     pickle.dump(episodes, ep)\n",
    "# # checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: opink. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Study and Movies\\程序Pythonic学习随笔痕迹\\from_alphazero_back_to_bandits\\trainmodels\\trywandb\\wandb\\run-20230119_144553-1rhkj75m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/opink/Muzero-15-15-5ziqi/runs/1rhkj75m\" target=\"_blank\">202301191445</a></strong> to <a href=\"https://wandb.ai/opink/Muzero-15-15-5ziqi\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_random =  [(-1, 1), (0, 0), (1, 99)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:29<49:08, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 0  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:40<39:12, 24.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [01:18<45:37, 28.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [01:58<50:55, 31.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [02:41<55:48, 35.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [03:19<56:32, 36.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [03:52<54:30, 35.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [04:37<58:20, 38.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [05:18<59:07, 38.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [05:43<51:55, 34.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [06:25<54:51, 36.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [07:13<59:07, 40.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [07:47<55:45, 38.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [08:32<57:45, 40.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [08:55<49:38, 35.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [09:34<50:46, 36.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [10:03<47:10, 34.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [10:42<48:30, 35.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [11:23<50:16, 37.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19  generated =  [(-1, 10), (0, 0), (1, 10)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_loss 7.840249 v_loss 1.763480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [12:28<53:12, 39.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_random =  [(-1, 0), (0, 0), (1, 100)] sum =  [(-1, 1), (0, 0), (1, 199)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Study and Movies\\程序Pythonic学习随笔痕迹\\from_alphazero_back_to_bandits\\trainmodels\\trywandb\\trywandb-15-15-5ziqi_implement_train_on_cuda-MuZeroExample.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Study%20and%20Movies/%E7%A8%8B%E5%BA%8FPythonic%E5%AD%A6%E4%B9%A0%E9%9A%8F%E7%AC%94%E7%97%95%E8%BF%B9/from_alphazero_back_to_bandits/trainmodels/trywandb/trywandb-15-15-5ziqi_implement_train_on_cuda-MuZeroExample.ipynb#X14sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m sum = \u001b[39m\u001b[39m'\u001b[39m, \u001b[39msorted\u001b[39m(vs_random_sum\u001b[39m.\u001b[39mitems()))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Study%20and%20Movies/%E7%A8%8B%E5%BA%8FPythonic%E5%AD%A6%E4%B9%A0%E9%9A%8F%E7%AC%94%E7%97%95%E8%BF%B9/from_alphazero_back_to_bandits/trainmodels/trywandb/trywandb-15-15-5ziqi_implement_train_on_cuda-MuZeroExample.ipynb#X14sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m new_checkpoint_path \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mPATH \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mg \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m wandb\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_games_one_epoch\u001b[39m}\u001b[39;00m\u001b[39m_round_model.m5\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Study%20and%20Movies/%E7%A8%8B%E5%BA%8FPythonic%E5%AD%A6%E4%B9%A0%E9%9A%8F%E7%AC%94%E7%97%95%E8%BF%B9/from_alphazero_back_to_bandits/trainmodels/trywandb/trywandb-15-15-5ziqi_implement_train_on_cuda-MuZeroExample.ipynb#X14sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(net, new_checkpoint_path)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Study%20and%20Movies/%E7%A8%8B%E5%BA%8FPythonic%E5%AD%A6%E4%B9%A0%E9%9A%8F%E7%AC%94%E7%97%95%E8%BF%B9/from_alphazero_back_to_bandits/trainmodels/trywandb/trywandb-15-15-5ziqi_implement_train_on_cuda-MuZeroExample.ipynb#X14sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(wandb\u001b[39m.\u001b[39mconfi\u001b[39m.\u001b[39mPATH \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mepisodes.pt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m ep:\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Study%20and%20Movies/%E7%A8%8B%E5%BA%8FPythonic%E5%AD%A6%E4%B9%A0%E9%9A%8F%E7%AC%94%E7%97%95%E8%BF%B9/from_alphazero_back_to_bandits/trainmodels/trywandb/trywandb-15-15-5ziqi_implement_train_on_cuda-MuZeroExample.ipynb#X14sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(episodes, ep)\n",
      "File \u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 379\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    380\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:484\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    482\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[0;32m    483\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[1;32m--> 484\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[0;32m    485\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    486\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "# Main algorithm of MuZero\n",
    "from collections import deque \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "config = {\n",
    "\"num_filters\": 64, # NN:Resnet structure\n",
    "\"num_blocks\": 12,  # NN:Resnet structure\n",
    "\"optimizer\": 'SGD (with momentum)',\n",
    "\"learning_rate\": 0.0003,\n",
    "\"weight_decay\": 0.00003,\n",
    "\"momentum\": 0.9,\n",
    "\"batch_size\": 512,          # train:每次训练时采样的batch总数\n",
    "\"num_steps\": 100,           # train:每次训练时的epoch总数\n",
    "\"num_game\": 100,            # self-play:本次selfplay的总局数\n",
    "\"num_games_one_epoch\": 20,  # self-play:每selfplay多少轮，训练模型一次\n",
    "\"num_simulations\": 100,     # self-play:selfplay时每个Node的search搜索次数\n",
    "\"lr_scheduler\": 'torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=wandb.config.num_steps, eta_min=1e-5)',\n",
    "\"deque_maxlen\": 2000,\n",
    "\"start_π_temperature\": 1.0,\n",
    "\"π_temperature_decay\": 0.8,\n",
    "\"PATH\": '.model/checkpoints/{}/'.format(time.strftime('%Y%m%d%H%M'))\n",
    "}\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"Muzero-15-15-5ziqi\", entity=\"opink\", config=config, name=config[\"PATH\"].split('/')[-2])\n",
    "if not os.path.exists(wandb.config.PATH):\n",
    "    os.makedirs(wandb.config.PATH)\n",
    "\n",
    "num_games = wandb.config.num_game                      # selfplay的总局数\n",
    "num_games_one_epoch = wandb.config.num_games_one_epoch # 每selfplay多少轮，训练模型一次\n",
    "num_simulations = wandb.config.num_simulations         # 每个Node的search搜索次数\n",
    "\n",
    "# load net parameters train&save on GPU\n",
    "# net = Net()\n",
    "# net.load_state_dict(torch.load(PATH + '1000r.pt'))\n",
    "# net.cuda()\n",
    "\n",
    "net = Net().cuda() # initial training\n",
    "wandb.watch(net, log='all')\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=3e-4, weight_decay=3e-5, momentum=0.9)\n",
    "schedular = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,T_max=wandb.config.num_steps, eta_min=1e-5)\n",
    "\n",
    "# Display battle results as {-1: lose 0: draw 1: win} (for episode generated for training, 1 means that the first player won)\n",
    "vs_random_sum = vs_random(net)\n",
    "print('vs_random = ', sorted(vs_random_sum.items()))\n",
    "\n",
    "# episodes = []\n",
    "episodes = deque([], maxlen=5000) # 移动队列用于存储最新的模型游戏\n",
    "\n",
    "result_distribution = {1: 0, 0: 0, -1: 0}\n",
    "\n",
    "for g in tqdm(range(num_games)):\n",
    "    # Generate one episode\n",
    "    record, p_targets, features, action_features = [], [], [], []\n",
    "    state = State()\n",
    "    # temperature using to make policy targets from search results\n",
    "    temperature = 1.0\n",
    "\n",
    "    tree = Tree(net) # 每轮游戏维护同一个树\n",
    "\n",
    "    while not state.terminal():\n",
    "        # tree = Tree(net) # 有必要每走一步都新建一个搜索树么？移到循环外面可以么？\n",
    "        p_target = tree.think(state, num_simulations, temperature)\n",
    "        p_targets.append(p_target)\n",
    "        features.append(state.feature())\n",
    "\n",
    "        # Select action with generated distribution, and then make a transition by that action\n",
    "        action = np.random.choice(np.arange(len(p_target)), p=p_target)\n",
    "        # 数据增广\n",
    "        record.append(symmetrys(action)) \n",
    "        action_features.append(state.action_feature(action))\n",
    "        state.play(action)\n",
    "        temperature *= 0.8 # 这个温度参数感觉意义不明。。。？\n",
    "\n",
    "    # reward seen from the first turn player\n",
    "    reward = state.terminal_reward() * (1 if len(record) % 2 == 0 else -1)\n",
    "    result_distribution[reward] += 1\n",
    "    \n",
    "    # 增广轨迹\n",
    "    for idx in range(8):\n",
    "        enrich_record = [step[idx] for step in record]\n",
    "        episodes.append((enrich_record, reward, features, action_features, p_targets))\n",
    "\n",
    "    if g % num_games_one_epoch == 0:\n",
    "        print('game ', end='')\n",
    "    print(g, ' ', end='')\n",
    "\n",
    "    # Training of neural net\n",
    "    if (g + 1) % num_games_one_epoch == 0:\n",
    "        # Show the result distributiuon of generated episodes\n",
    "        print('generated = ', sorted(result_distribution.items()))\n",
    "        epi = list(episodes.copy())\n",
    "        net = train(episodes=epi, net=net, optimizer=optimizer)\n",
    "        schedular.step()\n",
    "        del epi\n",
    "        \n",
    "        vs_random_once = vs_random(net)\n",
    "        print('vs_random = ', sorted(vs_random_once.items()), end='')\n",
    "        for r, n in vs_random_once.items():\n",
    "            vs_random_sum[r] += n\n",
    "        print(' sum = ', sorted(vs_random_sum.items()))\n",
    "\n",
    "        new_checkpoint_path = wandb.config.PATH + f'{g // wandb.config.num_games_one_epoch}_round_model.pt'\n",
    "        torch.save(net.state_dict(), new_checkpoint_path)\n",
    "        with open(wandb.confi.PATH + 'episodes.pt', 'wb') as ep:\n",
    "            pickle.dump(episodes, ep)\n",
    "# 上传wandb.artifacts训练后的模型\n",
    "eval_artifact = wandb.Artifact(type='model', name='run-%s-%s' % (wandb.config.PATH.split('/')[-2], wandb.config.num_game))\n",
    "eval_artifact.add_dir('.model/checkpoints/')\n",
    "# eval_artifact.add_file(new_checkpoint_path)\n",
    "wandb.run.log_artifact(eval_artifact)\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_checkpoint_path = f'.model/checkpoints/1000r.pt'\n",
    "torch.save(net.state_dict(), new_checkpoint_path)\n",
    "\n",
    "# related tips\n",
    "# 4. Trained and Save on GPU, Load on GPU\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# \n",
    "# When loading a model on a GPU that was trained and saved on GPU, \n",
    "# simply\n",
    "# convert the initialized model to a CUDA optimized model using\n",
    "# ``model.to(torch.device('cuda'))``.\n",
    "# \n",
    "# Be sure to use the ``.to(torch.device('cuda'))`` function \n",
    "# on all model inputs \n",
    "# to prepare the data for the model.\n",
    "\n",
    "# PATH = \"model.pt\"\n",
    "# net.cuda()\n",
    "# # Save\n",
    "# torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# # Load\n",
    "# device = torch.device(\"cuda\")\n",
    "# model = Net()\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 2 3 4 5 6 7 8 9\n",
      "A _ _ _ _ _ _ _ _ _\n",
      "B _ _ _ _ _ _ _ _ _\n",
      "C _ _ _ _ _ _ _ _ _\n",
      "D _ _ _ _ _ _ _ _ _\n",
      "E _ _ _ O O O O _ _\n",
      "F _ _ _ _ X X X _ _\n",
      "G _ _ _ _ _ _ _ _ _\n",
      "H _ _ _ _ _ _ _ _ _\n",
      "I _ _ _ _ _ _ _ _ _\n",
      "record = E4 F5 E5 F6 E6 F7 E7\n",
      "1.00 sec. best D8. q = -0.6277. n = 32 / 245. pv = D8\n",
      "2.00 sec. best D8. q = -0.6442. n = 38 / 414. pv = D8\n",
      "3.00 sec. best D8. q = -0.6584. n = 47 / 552. pv = D8\n",
      "4.00 sec. best D8. q = -0.6583. n = 61 / 675. pv = D8\n",
      "5.00 sec. best D8. q = -0.6892. n = 75 / 797. pv = D8\n",
      "6.00 sec. best D8. q = -0.6955. n = 89 / 956. pv = D8\n",
      "7.00 sec. best D8. q = -0.7111. n = 99 / 1104. pv = D8\n",
      "8.01 sec. best D8. q = -0.7193. n = 109 / 1233. pv = D8\n",
      "9.00 sec. best D8. q = -0.7177. n = 119 / 1345. pv = D8\n",
      "10.01 sec. best D8. q = -0.7185. n = 129 / 1463. pv = D8\n",
      "11.00 sec. best D8. q = -0.7191. n = 141 / 1588. pv = D8\n",
      "12.00 sec. best D8. q = -0.7202. n = 145 / 1732. pv = D8\n",
      "13.00 sec. best D8. q = -0.7243. n = 149 / 1882. pv = D8\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Search with trained net\n",
    "\n",
    "tree = Tree(net)\n",
    "state = State()\n",
    "\n",
    "next_step = tree.think(state.play('E4 F5 E5 F6 E6 F7 E7'), 2000, show=True)\n",
    "print(next_step.reshape((9 , 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 2 3 4 5 6 7 8 9\n",
      "A _ _ _ _ _ _ _ _ _\n",
      "B _ _ _ _ _ _ _ _ _\n",
      "C _ _ _ _ _ _ _ _ _\n",
      "D _ _ _ _ _ _ _ _ _\n",
      "E _ _ _ O O O O _ _\n",
      "F _ _ _ _ X X X X _\n",
      "G _ _ _ _ _ _ _ _ _\n",
      "H _ _ _ _ _ _ _ _ _\n",
      "I _ _ _ _ _ _ _ _ _\n",
      "record = E4 F5 E5 F6 E6 F7 E7 F8\n",
      "1.00 sec. best E8. q = 0.9508. n = 92 / 203. pv = E8\n",
      "2.00 sec. best E8. q = 0.9554. n = 148 / 381. pv = E8\n",
      "3.01 sec. best E8. q = 0.9560. n = 207 / 528. pv = E8\n",
      "4.01 sec. best E8. q = 0.9579. n = 253 / 667. pv = E8\n",
      "是否已经终局:  False\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "next_step = tree.think(state.play('F8'), 800, show=True)\n",
    "print('是否已经终局: ', state.terminal())\n",
    "print(next_step.reshape((9 , 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 2 3 4 5 6 7 8 9\n",
      "A _ _ _ _ _ _ _ _ _\n",
      "B _ _ _ _ _ _ _ _ _\n",
      "C _ _ _ _ _ _ _ _ _\n",
      "D _ _ _ _ _ _ _ _ _\n",
      "E _ _ _ O O O O O _\n",
      "F _ _ _ _ X X X X _\n",
      "G _ _ _ _ _ _ _ _ _\n",
      "H _ _ _ _ _ _ _ _ _\n",
      "I _ _ _ _ _ _ _ _ _\n",
      "record = E4 F5 E5 F6 E6 F7 E7 F8 E8\n",
      "1.00 sec. best F9. q = -0.8868. n = 36 / 189. pv = F9\n",
      "2.01 sec. best F9. q = -0.8929. n = 62 / 330. pv = F9\n",
      "3.01 sec. best F9. q = -0.8966. n = 76 / 449. pv = F9\n",
      "4.00 sec. best F9. q = -0.9030. n = 91 / 548. pv = F9\n",
      "5.01 sec. best F9. q = -0.9080. n = 107 / 640. pv = F9\n",
      "6.01 sec. best F9. q = -0.9083. n = 120 / 726. pv = F9\n",
      "是否已经终局:  True\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "next_step = tree.think(state.play('E8'), 800, show=True)\n",
    "print('是否已经终局: ', state.terminal())\n",
    "print(next_step.reshape((9 , 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
